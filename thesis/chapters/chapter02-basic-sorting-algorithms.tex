\chapter{Przegląd podstawowych algorytmów sortujących}
\thispagestyle{chapterBeginStyle}


\section{Quick Sort}
\input{chapters/tables/quick-sort}

Historia algorytmu Quick Sort sięga drugiej połowy XX wieku. W roku 1959 brytyjski naukowiec Tony Hoare
opracował, a dwa lata później opublikował pierwszą wersję tego algorytmu. Od tamtego czasu powstało wiele
udoskonaleń tego algorytmu, jednak jego koncepcja nadal jest widoczna we współczesnych językach programowania\footnote{Dokumentacja biblioteki sortującej w języku java: \url{https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/util/Arrays.html}}.
Na cześć algorytmu Quick Sort standardowa funkcja sortująca w języku C++ nosi nazwę qsort\footnote{Dokumentacja funkcji sortującej qsort:
\url{https://en.cppreference.com/w/cpp/algorithm/qsort}}.\\

Algorytm Quick Sort składa się z dwóch etapów. Pierwszym z nich jest partycjonowanie zbioru wejściowego.
Po tym kroku tablica wejściowa jest rozbita na dwa rozłączne zbiory, w których wszystkie elementy pierwszego
zbioru są skumulowane po lewej stronie tablicy oraz każdy z tych elementów jest większy od dowolnego elementu
z drugiej tablicy. Drugim etapem jest rekurencyjne sortowanie lewej oraz prawej podtablicy.
Algorytm Quick Sort wykorzystuje technikę dziel i zwyciężaj, ponieważ problem sortowania tablicy wejściowej
rozbija na sortowanie dwóch podtablic.

% TODO: Tabelska sortowania
% TODO: ile procentowo mniej porownan wykonuje w przypadku optymistycznym w porownaniu do przypadku średniego

\subsection{Analiza algorytmu Quick Sort}

Liczba operacji wykonywanych przez algorytm Quick Sort została przeanalizowana pod kątem trzech przypadków:
optymistycznego, średniego oraz pesymistycznego.\\

Dla algorytmu Quick Sort przypadek optymistyczny (\ref{fig:quick-sort-optimistic-average}) następuje wówczas, gdy algorytm partycjonowania przy każdym wywołaniu dzieli tablicę wejściową na dwie równe części. Efekt ten uzyskano, wprowadzając dane już posortowane oraz stosując algorytm wybierający piwot dokładnie w połowie tablicy. Z analizy eksperymentalnej wynika, że w przypadku optymistycznym algorytm działa ze złożonością czasową $O(n\log{}n)$.\\

Przypadek pesymistyczny (\ref{fig:quick-sort-pessimistic}) zachodzi, gdy drzewo wowołań rekurencyjnych jest możliwie najgłębsze. Efekt ten uzyskano, wprowadzając dane już posortowane oraz stosując algorytm wybierający piwot jako ostatni element tablicy. W tej sytuacji w kolejnych iteracjach rozpatrywana jest tablica z rozmiarem o jeden mniejszy od poprzedniej, a więc drzewo wywołań rekurencyjnych ma głębokość $n$. Z analizy wynika, że złożoność czasowa algorytmu w przypadku pesymistycznym wynosi $O(n^2)$.\\

Przypadek średni (\ref{fig:quick-sort-optimistic-average}) został zbadany wprowadzając losowe dane z powtórzeniami oraz stosując algorytm wybierający piwot jako ostatni element tablicy. Analiza eksperymentalna wykazała, że w przypadku średnim algorytm Quick Sort ma złożoność czasową równą $O(n\log{}n)$, a więc jest tego samego rzędu co dla przypadku optymistycznego.\\

Porównując liczbę wykonywanych operacji można zauważyć, że algorytm Quick Sort wykonuje prawie dwa więcej operacji porównania
niż operacji zamiany miejsc. Liczba pojedynczych operacji przypisania rośnie liniowo, a więc jest znikoma w porównaniu z
liczbą pozostałych operacji.\\

Analizując rozkład prawdopodobieństwa liczby wykonanych operacji (\ref{fig:quick-sort-density}) użyto tablicy losowych danych o stałym rozmiarze $n = 10000$. Można zauważyć, że liczba operacji porównania oraz liczba operacji zamiany miejsc nie są przedstawiane za pomocą rozkładu normalnego. Bardziej prawdopodobne jest wykonanie większej liczby tych operacji w stosunku do wartości średniej. Z kolei rozkład liczby wykonanych operacji przypisania przedstawia się za pomocą rozkładu normalnego, z jednakowym prawdopodobieństwem liczba ta może być większa lub mniejsza od wartości średniej.\\

\begin{figure}[H]
	\centering
	\includesvg[inkscapelatex=false,width=1.0\columnwidth]{img/plot/quick-sort-optimistic-average-case.svg}
	\caption[]{}
	\label{fig:quick-sort-optimistic-average}
\end{figure}

\begin{figure}[H]
	\centering
	\includesvg[inkscapelatex=false,width=1.0\columnwidth]{img/plot/quick-sort-pessimistic-case.svg}
	\caption[]{}
	\label{fig:quick-sort-pessimistic}
\end{figure}

\begin{figure}[H]
	\centering
	\includesvg[inkscapelatex=false,width=1.0\columnwidth]{img/plot/quick-sort-density.svg}
	\caption[]{}
	\label{fig:quick-sort-density}
\end{figure}

\subsection{Problemy związane z algorytmem Quick Sort}
Głównym problemem algorytmu Quick Sort jest jego słaba pesymistyczna złożoność czasowa. Ponieważ algorytm działa rekurencyjnie, w przypadku pesymistycznym głębokość drzewa wywołań rekurencyjnych może przekroczyć maksymalną liczbę ramek stosu, powodując awaryjne zatrzymanie programu.\\

Kolejnym problemem tego algorytmu jest stosunkowo duża liczba wykonywanych operacji porównania w stosunku do liczby pozostałych operacji. Punkt ten jest szczególnie istotny w sytuacji, gdy sortowane są złożone struktury, dla których
wykonanie pojedynczej operacji porównania jest znacznie kosztowniejsze od pozostałych operacji. W tym przypadku bardziej wskazanym wydaje się użycie algorytmu Merge Sort, którego analizę przeprowadzono w kolejnym rozdziale.

\subsection{Możliwości optymalizacyjne}
Ponieważ złożoność czasowa algorytmu Quick Sort uwarunkowana jest poprzez złożoność algorytmu partycjonowania, optymalizacja algorytmu może opierać się ulepszanie algorytmu partycjonowania, aby jak najefektywniej wyszukiwał piwot, zapewniając podział tablicy wejściowej na dwie tablice o zbliżonej długości.
 
% Tabelka sortowania

\section{Merge Sort}
\input{chapters/tables/merge-sort}

Algorytm Merge Sort został opracowany przez Johna von Neumanna w 1945 roku. Tak jak Quick Sort, algorytm ten wykorzystuje
technikę dziel i zwyciężaj aby rekurencyjnie rozbić problem sortowania danych wejściowych na dwie listy mniejszej długości.
W przeciwieństwie do algorytmu Quick Sort, algorytm Merge Sort do poprawnego działania potrzebuje dodatkowej pamięci.
W rozważanej implementacji algorytm alokuje dodatkowy bufor długości $n/2$.\\

Algorytm Merge Sort składa się z trzech etapów. Pierwszym krokiem jest podział tablicy wejściowej na dwie części o zbliżonej długości. Drugim etapem jest rekurencyjne sortowanie każdej z części. Ostatnim krokiem jest scalenie tablic cząstkowych zgodnie z porządkiem sortowania. Aby efektywnie wykonać ostatni krok, algorytm Merge Sort potrzebuje zewnętrznego bufora.\\

\subsection{Analiza algorytmu Merge Sort}
Liczba operacji wykonywanych przez algorytm Merge Sort została przeanalizowana pod kątem trzech przypadków:
optymistycznego, średniego oraz pesymistycznego.\\

Dla algorytmu Merge Sort przypadek optymistyczny (\ref{fig:merge-sort-optimistic-average-pessimistic-case}) następuje wówczas, gdy w każdym kroku scalania lewa podtablica zawiera tylko elementy mniejsze od wszystkich elementów z prawej podtablicy. Efekt ten uzyskano, wprowadzając dane już posortowane. Z analizy eksperymentalnej wynika, że w tym przypadku
algorytm Merge Sort działa ze złożonością czasową rzędu $O(n\log{}n)$.\\

Przypadek pesymistyczny (\ref{fig:merge-sort-optimistic-average-pessimistic-case}) zachodzi, gdy liczba porównać między 
sobą elementów w trakcie scalania jest możliwie największa. Efekt ten uzyskano, preparując dane wejściowe w taki sposób,
aby w każdym kroku scalania porównywane tablice zawierały elementy na przemian większe oraz mniejsze. Z analizy wynika, że złożoność czasowa algorytmu w przypadku pesymistycznym jest równa $O(n\log{}n)$, a więc jest tego samego rzędu co złożoność optymistyczna.\\

Przypadek średni (\ref{fig:merge-sort-optimistic-average-pessimistic-case}) zbadano wprowadzając na wejście losowe dane z powtórzeniami. Analiza wykazała, że w przypadku średnim złożoność algorytmu jest równa $O(n\log{}n)$.\\

W przeciwieństwie do algorytmu Quick Sort, algorytm Merge Sort wykonuje dokładnie tyle samo operacji porównania co operacji
przypisania. Jest to zgodne z rzeczywistością, ponieważ w trakcie scalania każdej operacji porównania towarzyszy przypisanie wartości do bufora. Można zauważyć, że algorytm Merge Sort w rozpatrywanej postaci nie wykonuje operacji zamiany miejsc.\\

Do analizy rozkładu prawdopodobieństwa liczby wykonanych operacji (\ref{fig:merge-sort-density}) użyto tablicy losowych danych o stałym rozmiarze $n = 1000$. W analizie pominięto liczbę operacji przypisania, która jest zerowa dla każdego przypadku. Analiza eksperymentalna dowodzi, że w przeciwieństwie do algorytmu Quick Sort, liczba operacji porównania dla algorytmu Merge Sort tworzy rozkład normalny, a więc prawdopodobieństwa otrzymania większej oraz mniejszej liczby operacji przypisania są jednakowe. Ponieważ dla badanego algorytmu, liczba operacji przypisania jest równa liczbie operacji porównania, rozkłady tych wartości są jednakowe.

\begin{figure}[H]
	\centering
	\includesvg[inkscapelatex=false,width=1.0\columnwidth]{img/plot/merge-sort-optimistic-average-pessimistic-case.svg}
	\caption[]{}
	\label{fig:merge-sort-optimistic-average-pessimistic-case}
\end{figure}

\begin{figure}[H]
	\centering
	\includesvg[inkscapelatex=false,width=0.66\columnwidth]{img/plot/merge-sort-density.svg}
	\caption[]{}
	\label{fig:merge-sort-density}
\end{figure}

\subsection{Problemy związane z algorytmem Merge Sort}
Głównym problemem algorytmu Merge Sort jest konieczność alokacji dodatkowego bufora pamięci. W rozważanej implementacji algorytm alokuje dodatkowy bufor długości $n/2$, a więc może okazać się bezużyteczny dla systemów z ograniczonym zasobem pamięci.\\

\subsection{Możliwości optymalizacyjne}
% byc moze optymalizacja moze polegac na ulepszeniu algorytmu mergujacego
Jednym ze sposobów na optymalizację algorytmu Merge Sort jest próba przekształcenia go w algorytm działający w miejscu.
Cel ten może zostać osiągnięty poprzez skrzyżowanie algorytmu Merge Sort z innym algorytmem sortującym w taki sposób, aby bufor dodatkowej pamięci był częścią tablicy wejściowej. Rozwiązanie to zostanie przeanalizowane w dalszych rozdziałach.\\
